# @package diffing.method
name: sae_difference

# Training parameters - inherits data config from preprocessing
training:
  target: "difference_ftb"  # ["difference_bft", "difference_ftb"] - which difference to compute
  expansion_factor: 32
  batch_size: 2048
  epochs: 1
  lr: 1e-3
  max_steps: null  # Auto-calculate from dataset size
  validate_every_n_steps: 10000
  k: 100   # Sparsity for batch-top-k SAE
  
  # Data configuration
  num_samples: 100_000_000
  num_validation_samples: 2_000_000
  local_shuffling: true
  local_shuffling_shard_size: 1_000_000
  workers: 16
  overwrite: false
  
datasets:
  use_chat_dataset: true
  use_pretraining_dataset: true
  use_training_dataset: true
  # Normalization configuration for difference computation
  normalization:
    enabled: true
    subsample_size: 1_000_000  # Number of samples to use for std computation
    batch_size: 4096
    cache_dir: "${infrastructure.storage.base_dir}/diff_normalizer"

# Model configuration - only BatchTopK supported
model:
  type: "batch-top-k"  # Only batch-top-k supported for SAE difference

# Training optimization
optimization:
  resample_steps: null
  warmup_steps: 1000

layers: null  # Fraction of model layers to train on, if null, train on all available layers. Provide list of layers to train on.

# Analysis configuration
analysis:
  enabled: true

  latent_scaling:
    enabled: true
    dataset_split: "train"
    targets: ["base_activation", "ft_activation"]
    num_samples: 1_000_000
    batch_size: 128
    num_workers: 4
    device: "cuda"
    dtype: "float32"
    num_effective_ft_only_latents: 5000
    overwrite: false
    
  latent_activations: # Collect latent activations for all datasets and layers
    enabled: true
    split: "validation"
    overwrite: false # Overwrite existing latent activations and max activations
    upload_to_hub: false # Upload max activations to hub
    n_max_activations: 100 # Number of max activations to collect
    min_threshold: 1e-4 # Minimum activation threshold to consider
    max_num_samples: 10000 # Maximum number of samples to collect per dataset

upload:
  model: false