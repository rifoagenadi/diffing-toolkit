# @package diffing.method
name: crosscoder

# Training parameters - inherits data config from preprocessing
training:
  expansion_factor: 32
  batch_size: 2048
  epochs: 2
  lr: 1e-3
  max_steps: null  # Auto-calculate from dataset size
  validate_every_n_steps: 10000
  mu: 1e-1  # L1 penalty for ReLU type
  k: 100   # Sparsity for batch-top-k type
  k_max: 2048
  k_annealing_steps: 0
  auxk_alpha: 0.03125  # 1/32
  
  # Data configuration
  num_samples: 100_000_000
  num_validation_samples: 2_000_000
  local_shuffling: true
  local_shuffling_shard_size: 1_000_000
  workers: 16
  normalize_activations: true
  
datasets:
  use_chat_dataset: true
  use_pretraining_dataset: true
  use_training_dataset: true
  normalize_activations: true

# Model configuration
model:
  type: "batch-top-k"  # ["relu", "batch-top-k"]
  code_normalization: "crosscoder"  # ["crosscoder", "sae", "mixed", "decoupled", "none"]
  same_init_for_all_layers: true
  norm_init_scale: 1.0
  init_with_transpose: true
  
# Training optimization
optimization:
  resample_steps: null
  warmup_steps: 1000

layers: null  # Fraction of model layers to train on, if null, train on all available layers. Provide list of layers to train on.

# Analysis configuration
analysis:
  enabled: true

  latent_scaling:
    enabled: true
    dataset_split: "train"
    targets: ["base_error", "ft_error", "base_reconstruction", "ft_reconstruction", "base_activation", "ft_activation"]
    num_samples: 50_000_000
    batch_size: 128
    num_workers: 4
    device: "cuda"
    dtype: "float32"
    num_effective_ft_only_latents: 5000

  latent_activations: # Collect latent activations for all datasets and layers
    enabled: true
    split: "validation"
    overwrite: false # Overwrite existing latent activations and max activations
    upload_to_hub: false # Upload max activations to hub
    n_max_activations: 100 # Number of max activations to collect
    min_threshold: 1e-4 # Minimum activation threshold to consider
    max_num_samples: 10000 # Maximum number of samples to collect per dataset


  # Analysis parameters
  skip_notebook: false
  skip_recon_scalars: false
  skip_error_scalars: false
  compute_latent_stats: true
  run_kl_experiment: true
  skip_token_level_replacement: false
  
  # Analysis parameters
  num_samples_betas: 50_000_000
  batch_size_kl: 6
  kl_dataset: "science-of-finetuning/lmsys-chat-1m-chat-formatted"
  kl_dataset_col: "conversation"
  kl_dataset_split: "validation"
  lmsys_col: null  # Optional column specification

# Hugging Face upload 
upload:
  df: true
  model: true
  
# Output configuration
output:
  save_trained_models: true
  save_training_metrics: true
  save_reconstruction_stats: true 