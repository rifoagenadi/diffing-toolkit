# @package diffing.method
name: kl_divergence

# Method parameters
method_params:
  batch_size: 8
  max_samples: 1000  # Process entire dataset if None
  max_tokens_per_sample: 1024
  temperature: 1.0  # Temperature for KL computation
  ignore_padding: true
  
datasets:
  use_chat_dataset: true
  use_pretraining_dataset: true
  use_training_dataset: true

# Analysis configuration  
analysis:
  # Only compute per-token KL divergence
  compute_per_token: true
  
  # Statistical summaries
  statistics:
    - mean
    - std
    - median
    - percentiles: [25, 75, 90, 95, 99]
    - max
    - min
    
  # Max activating examples
  max_activating_examples:
    num_examples: 100  # Number of max activating examples to export
    include_full_messages: true
    include_all_token_kls: true
    
# Output configuration
output:
  save_raw_divergences: false  # Can be memory intensive
  save_aggregated_stats: true
  save_max_examples: true
  output_format: ["json", "pt"]  # JSON for stats, PT for tensors
