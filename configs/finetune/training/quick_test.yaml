# @package finetune.training
name: quick_test

# Training hyperparameters (faster for testing)
learning_rate: 5e-5
batch_size: 8
gradient_accumulation_steps: 2
num_epochs: 1
warmup_steps: 50
weight_decay: 0.01
max_grad_norm: 1.0

# Optimizer settings
optimizer:
  name: adamw
  betas: [0.9, 0.999]
  eps: 1e-8

# Scheduler settings
scheduler:
  name: linear
  warmup_ratio: 0.1

# Training configuration
training_config:
  evaluation_strategy: steps
  eval_steps: 50
  save_strategy: steps
  save_steps: 50
  logging_steps: 10
  save_total_limit: 2
  load_best_model_at_end: false
  max_steps: 100  # Limit for quick testing

# Data loading
dataloader:
  num_workers: 2
  pin_memory: true
  drop_last: false

# Testing specific
testing:
  use_subset: true
  subset_size: 1000  # Use only subset of data 